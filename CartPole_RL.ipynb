{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "81da33b1-622e-4cfa-ad83-c9b9817aed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "import gym\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    return None\n",
    "\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86bfda-6024-4ff7-bf68-abd1e1b1f0a1",
   "metadata": {},
   "source": [
    "## Code to Collect and Analyze Memory in CartPole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de25fb85-8583-491f-aa6a-c8da7e098e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 100 episodes: 21.15\n"
     ]
    }
   ],
   "source": [
    "# Create a new CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Number of episodes to run\n",
    "num_episodes = 100\n",
    "\n",
    "# Initialize a list to store the memory of all episodes\n",
    "life_memory = []\n",
    "\n",
    "# Loop through each episode\n",
    "for i in range(num_episodes):\n",
    "    # Start a new episode and reset the environment\n",
    "    old_observation, _ = env.reset()  # Get the initial observation and additional info (if any)\n",
    "    done = False  # Flag to indicate if the episode is finished\n",
    "    tot_reward = 0  # Variable to accumulate the total reward for the episode\n",
    "    ep_memory = []  # List to store the memory of the current episode\n",
    "    \n",
    "    # Run the episode until it finishes\n",
    "    while not done:\n",
    "        # Take a random action from the action space\n",
    "        new_action = env.action_space.sample()\n",
    "        \n",
    "        # Step the environment using the chosen action\n",
    "        # Unpack the values returned from env.step()\n",
    "        observation, reward, done, truncated, info = env.step(new_action)\n",
    "        \n",
    "        # Update the total reward for this episode\n",
    "        tot_reward += reward\n",
    "        \n",
    "        # Ensure that the observation has the expected shape (4 values for CartPole)\n",
    "        if len(old_observation) != 4:\n",
    "            print(f\"Unexpected shape of old_observation: {old_observation}\")\n",
    "            break  # Exit the loop if the observation shape is unexpected\n",
    "        \n",
    "        # Record the memory of the current step\n",
    "        ep_memory.append({\n",
    "            \"obs0\": old_observation[0],  # Position of the cart\n",
    "            \"obs1\": old_observation[1],  # Velocity of the cart\n",
    "            \"obs2\": old_observation[2],  # Angle of the pole\n",
    "            \"obs3\": old_observation[3],  # Angular velocity of the pole\n",
    "            \"action\": new_action,        # Action taken (0 or 1)\n",
    "            \"reward\": reward,            # Reward received for this step\n",
    "            \"episode\": i,                # Current episode number\n",
    "        })\n",
    "        \n",
    "        # Update the old observation to the current observation\n",
    "        old_observation = observation\n",
    "    \n",
    "    # Add the total reward to each memory entry in the episode\n",
    "    for ep_mem in ep_memory:\n",
    "        ep_mem[\"tot_reward\"] = tot_reward  # Assign total reward for the episode\n",
    "    \n",
    "    # Extend the life memory with the current episode's memory\n",
    "    life_memory.extend(ep_memory)\n",
    "\n",
    "# Convert the collected memory into a Pandas DataFrame\n",
    "memory_df = pd.DataFrame(life_memory)\n",
    "\n",
    "# Compute the average reward per episode\n",
    "average_reward = memory_df.groupby(\"episode\").reward.sum().mean()\n",
    "\n",
    "# Print the average reward\n",
    "print(f\"Average reward over {num_episodes} episodes: {average_reward}\")\n",
    "\n",
    "# Close the environment to free resources\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a80f8-98e9-4e09-b95f-d0d87d71f217",
   "metadata": {},
   "source": [
    "## Training Multiple Regression Models on Reinforcement Learning Memory Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fdf39289-73cf-41db-a1e1-8e61b38efcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of models to train\n",
    "# - ExtraTreesRegressor: A tree-based model that uses an ensemble of randomized decision trees.\n",
    "# - AdaBoostRegressor: A boosting algorithm that combines weak learners to create a strong learner.\n",
    "# - ExtraTreesRegressor: Added another instance of ExtraTreesRegressor for experimentation.\n",
    "models = [\n",
    "    ExtraTreesRegressor(n_estimators=50),  # First ExtraTrees model\n",
    "    AdaBoostRegressor(n_estimators=50),   # AdaBoost model\n",
    "    ExtraTreesRegressor(n_estimators=50)  # Second ExtraTrees model\n",
    "]\n",
    "\n",
    "# Create a new column `comb_reward` in the memory DataFrame\n",
    "# - This column is a weighted combination of step-wise rewards (`reward`) \n",
    "#   and the total reward for the episode (`tot_reward`).\n",
    "memory_df[\"comb_reward\"] = 0.5 * memory_df.reward + memory_df.tot_reward\n",
    "\n",
    "# Loop through each model in the list and train it\n",
    "for model in models:\n",
    "    # Fit the model to the training data\n",
    "    # - Input features: `obs0`, `obs1`, `obs2`, `obs3` (state observations) and `action`.\n",
    "    # - Target variable: `comb_reward` (the combined reward we defined earlier).\n",
    "    model.fit(\n",
    "        memory_df[[\"obs0\", \"obs1\", \"obs2\", \"obs3\", \"action\"]],  # Features\n",
    "        memory_df.comb_reward                                   # Target\n",
    "    )\n",
    "    # Once fitted, the model can be used to predict rewards or guide decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ae7ff-e324-45b3-a79c-5833bee5dd6c",
   "metadata": {},
   "source": [
    "## Evaluation of Models in the CartPole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9135fb42-d0e6-4e6d-a430-e796c6c39a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: Average reward over 100 episodes: 122.88\n",
      "Model 2: Average reward over 100 episodes: 9.29\n",
      "Model 3: Average reward over 100 episodes: 99.82\n"
     ]
    }
   ],
   "source": [
    "# Evaluation setup\n",
    "num_episodes = 100  # Number of episodes to evaluate each model\n",
    "random_per = 0      # Probability of taking a random action (0 means always using the model's prediction)\n",
    "results = {}        # Dictionary to store the average reward for each model\n",
    "\n",
    "# Evaluate each model\n",
    "for model_idx, model in enumerate(models):\n",
    "    # Reset life memory for the current model\n",
    "    life_memory = []\n",
    "    \n",
    "    # Create a new CartPole environment for each model's evaluation\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    # Loop through the number of episodes\n",
    "    for i in range(num_episodes):\n",
    "        # Reset the environment and get the initial state\n",
    "        old_observation, _ = env.reset()\n",
    "        done = False  # Flag to indicate if the episode is done\n",
    "        tot_reward = 0  # Total reward for the episode\n",
    "        ep_memory = []  # Store episode-level memory\n",
    "\n",
    "        # Loop until the episode is done\n",
    "        while not done:\n",
    "            # Ensure the observation is in the correct shape (1D array)\n",
    "            old_observation = np.array(old_observation).flatten()\n",
    "            \n",
    "            # Decide the action\n",
    "            if np.random.rand() < random_per:\n",
    "                # Take a random action with probability `random_per`\n",
    "                new_action = env.action_space.sample()\n",
    "            else:\n",
    "                # Use the model to predict the best action\n",
    "                # Create input data for prediction (current state + possible actions)\n",
    "                pred_in = [list(old_observation) + [i] for i in range(2)]  # Actions: 0 or 1\n",
    "                pred_in = np.array(pred_in)  # Ensure consistent 2D shape\n",
    "                new_action = np.argmax(model.predict(pred_in))  # Predict the best action\n",
    "            \n",
    "            # Take the chosen action in the environment\n",
    "            observation, reward, done, truncated, info = env.step(new_action)\n",
    "            tot_reward += reward  # Update total reward\n",
    "            \n",
    "            # Save the transition data to episode memory\n",
    "            ep_memory.append({\n",
    "                \"obs0\": old_observation[0],\n",
    "                \"obs1\": old_observation[1],\n",
    "                \"obs2\": old_observation[2],\n",
    "                \"obs3\": old_observation[3],\n",
    "                \"action\": new_action,\n",
    "                \"reward\": reward,\n",
    "                \"episode\": i,\n",
    "            })\n",
    "            \n",
    "            # Update the observation\n",
    "            old_observation = observation\n",
    "        \n",
    "        # Add the total reward to each step in the episode memory\n",
    "        for ep_mem in ep_memory:\n",
    "            ep_mem[\"tot_reward\"] = tot_reward\n",
    "        \n",
    "        # Append the episode memory to life memory\n",
    "        life_memory.extend(ep_memory)\n",
    "\n",
    "    # Create a DataFrame from the collected memory of all episodes for this model\n",
    "    memory_df2 = pd.DataFrame(life_memory)\n",
    "    \n",
    "    # Recalculate the combined reward for evaluation purposes\n",
    "    memory_df2[\"comb_reward\"] = memory_df2.reward + memory_df2.tot_reward\n",
    "\n",
    "    # Compute the average reward over all episodes for this model\n",
    "    avg_reward = memory_df2.groupby(\"episode\").reward.sum().mean()\n",
    "    results[f\"Model {model_idx + 1}\"] = avg_reward  # Store the result\n",
    "\n",
    "    # Print the average reward for the current model\n",
    "    print(f\"Model {model_idx + 1}: Average reward over {num_episodes} episodes: {avg_reward}\")\n",
    "\n",
    "# Close the environment to free up resources\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
